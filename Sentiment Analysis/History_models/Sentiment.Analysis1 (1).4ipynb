{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc6f1802-7711-4b7d-842d-4d90ab3feb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hatem\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: torch in c:\\users\\hatem\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\hatem\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hatem\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hatem\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hatem\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hatem\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch pandas numpy scipy tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbeb259a-3965-4852-bc7f-3fdd29843c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51de434-017e-4ac0-bf50-dbcd1c21e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68aa7fa-92be-4811-b472-e7f4bbc331ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Arabic model for sentiment analysis\n",
    "tokenizer_sa = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "model_sa = AutoModelForMaskedLM.from_pretrained(\"facebook/mbart-large-cc25\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b50a36-062a-4d5c-b7e1-cbff376dbdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer (for summarization)\n",
    "tokenizer_sum = AutoTokenizer.from_pretrained(\"aubmindlab/aragpt2-medium\")\n",
    "model_sum = AutoModelForCausalLM.from_pretrained(\"aubmindlab/aragpt2-medium\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349ffb39-fef0-472a-973e-ff9f92b9d46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Desktop/First_tes.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the Excel file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDesktop/First_tes.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(file_path)\n\u001b[0;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConcatenatedResponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[0;32m    496\u001b[0m         io,\n\u001b[0;32m    497\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    498\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[0;32m    499\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[0;32m   1552\u001b[0m     )\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1403\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Desktop/First_tes.xlsx'"
     ]
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "file_path = 'Desktop/First_tes.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df.columns = ['S_ID', 'ConcatenatedResponse']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e2e4ec-5e59-4535-968d-990acb442404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_arabic(text):\n",
    "    try:\n",
    "        encoded_text = tokenizer_sa(text, return_tensors='pt').to(device)\n",
    "        output = model_sa(**encoded_text)\n",
    "\n",
    "        # Ensure that output is in expected format\n",
    "        if hasattr(output, 'logits'):\n",
    "            scores = output.logits[0].detach().cpu().numpy()\n",
    "        else:\n",
    "            scores = output[0][0].detach().cpu().numpy()\n",
    "\n",
    "        scores = softmax(scores)\n",
    "\n",
    "        # Ensure scores are in correct order\n",
    "        if len(scores) >= 3:\n",
    "            scores_dict = {\n",
    "                'arabic_neg': scores[0],\n",
    "                'arabic_neu': scores[1],\n",
    "                'arabic_pos': scores[2]\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected number of score values\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}, error: {e}\")\n",
    "        scores_dict = {\n",
    "            'arabic_neg': 0.0,\n",
    "            'arabic_neu': 0.0,\n",
    "            'arabic_pos': 0.0\n",
    "        }\n",
    "    return scores_dict\n",
    "\n",
    "def summarize_text(text):\n",
    "    try:\n",
    "        inputs = tokenizer_sum.encode(\"summarize: \" + text, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "        summary_ids = model_sum.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer_sum.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing text: {text}, error: {e}\")\n",
    "        summary = \"Error in summarization\"\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03b595-f1fa-4832-85dd-511f200fd55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store individual results\n",
    "results = []\n",
    "individual_summaries = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09b463-04fb-4fc3-a384-3308dd953cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate through the rows of the DataFrame\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    sentiment_scores = polarity_scores_arabic(row['ConcatenatedResponse'])\n",
    "\n",
    "    # Ensure that sentiment_scores is a dictionary with valid scores\n",
    "    if isinstance(sentiment_scores, dict):\n",
    "        # Convert any array-like values to scalars\n",
    "        for sentiment, score in sentiment_scores.items():\n",
    "            if isinstance(score, np.ndarray):\n",
    "                # If score is an array, take the first element or use .item() to extract scalar\n",
    "                sentiment_scores[sentiment] = score.item() if score.size == 1 else float(score[0])\n",
    "\n",
    "        # Use max to find the sentiment with the highest score\n",
    "        predicted_sentiment = max(sentiment_scores, key=sentiment_scores.get)\n",
    "\n",
    "        # Map the predicted sentiment to a human-readable format\n",
    "        if predicted_sentiment == 'arabic_neg':\n",
    "            sentiment = 'سلبي'  # Negative\n",
    "        elif predicted_sentiment == 'arabic_neu':\n",
    "            sentiment = 'محايد'  # Neutral\n",
    "        else:\n",
    "            sentiment = 'إيجابي'  # Positive\n",
    "    else:\n",
    "        print(f\"Error: Sentiment scores format is invalid for row {index}: {sentiment_scores}\")\n",
    "        continue  # Skip this row if the format is incorrect\n",
    "\n",
    "    # Summarize the text\n",
    "    summary = summarize_text(row['ConcatenatedResponse'])\n",
    "\n",
    "    # Append the results to the list\n",
    "    results.append({\n",
    "        'S_ID': row['S_ID'],\n",
    "        'ConcatenatedResponse': row['ConcatenatedResponse'],\n",
    "        'التصنيف': sentiment,  # Classification\n",
    "        'الملخص': summary  # Summary\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f22905-4871-40e0-a612-a8185f9117fd",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Collect individual summaries\n",
    "    individual_summaries.append(summary)\n",
    "\n",
    "# Create a new DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the individual results to an Excel file\n",
    "output_file_path = 'E:/1.0.0.0.1 DEPI/Final Project/Docs/results.xlsx'\n",
    "results_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "# Combine all individual summaries into one comprehensive summary\n",
    "comprehensive_summary = summarize_text(' '.join(individual_summaries))\n",
    "\n",
    "# Save the comprehensive summary to a text file\n",
    "comprehensive_output_file_path = 'E:/1.0.0.0.1 DEPI/Final Project/Docs/comprehensive_summary.txt'\n",
    "with open(comprehensive_output_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(comprehensive_summary)\n",
    "\n",
    "# Display the comprehensive summary\n",
    "print(comprehensive_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3f0af-33dd-4586-acab-8091c04facd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "df = pd.read_excel('D:/Data/SQL/First_tes.xlsx')\n",
    "df.columns = ['S_ID', 'C_ID', 'ConcatenatedResponse', 'True_Sentiment']\n",
    "\n",
    "# Assuming 'True_Sentiment' is your ground truth labels\n",
    "# Create a function to get predictions\n",
    "def get_predictions(row):\n",
    "    sentiment_scores = polarity_scores_arabic(row['ConcatenatedResponse'])\n",
    "    predicted_sentiment = max(sentiment_scores, key=sentiment_scores.get)\n",
    "    if predicted_sentiment == 'arabic_neg':\n",
    "        return 'سلبي'\n",
    "    elif predicted_sentiment == 'arabic_neu':\n",
    "        return 'محايد'\n",
    "    else:\n",
    "        return 'إيجابي'\n",
    "\n",
    "# Get predictions\n",
    "df['Predicted_Sentiment'] = df.apply(get_predictions, axis=1)\n",
    "\n",
    "# Convert sentiments to numerical labels if needed\n",
    "label_map = {'سلبي': 0, 'محايد': 1, 'إيجابي': 2}\n",
    "y_true = df['True_Sentiment'].map(label_map)\n",
    "y_pred = df['Predicted_Sentiment'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1612e-3b3d-433e-b1e5-b29e930c4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Confusion matrix for more insights\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c4183-911d-46bf-b293-3e23fccdfe68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
